Block-Based Matrix Multiplication using Hadoop MapReduce & Apache Spark

we consider using Hadoop MapReduce and Spark to implement the block-based matrix multiplication. You can assume that each matrix is stored as a text file. Each line of the file corresponds to a block of the matrix in the format of index-content. You may assume all blocks are 2-by-2 and all matrices are 6-by-6. That is each matrix is divided into 9 blocks of equal size (2-by-2 or 4 elements). The content of block is stored in a sparse format: (row index, column index, value) where row and column indexes are relative to the block. All indexes start from 1.

For example, the following is a fragment of a file for the matrix A above.(1,1),[(1,1,2),(1,2,1),(2,2,3)](1,2),[(1,2,3),(2,1,2)]â€¦Note that the first line describes the block A11, 2nd line describes A12. Note also the format as illustrated (e.g., [] encloses the content of a block; no white spaces used; comma separates the numbers, etc.).

1.
Write a Hadoop MapReduce program, BlockMult.java, to multiply matrix A and B. Use one-phase approach and we use only one reducer here as described in class.Sample invocation:	hadoop jar bm.jar BlockMult file-A file-B output_pathWhere dir-A stores the content of matrix A in the format described above.

2.
Implement the same function as question 1 but use Spark in Python instead. Name your program: BlockMult.py. (You do not need output path as the specific directory here, you can just create a output.txt in your code).Sample invocation:	spark-submit BlockMult.py file-A file-B (output_path)